---
phase: 71-test-infrastructure-fix
plan: 02
type: execute
wave: 1
depends_on: ["71-01"]
files_modified:
  - tests/e2e/test_focus_loss_data_parity.py
  - tests/e2e/test_data_comparison.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "test_focus_loss_episode_boundary_parity passes when run immediately after test_data_comparison.py completes (back-to-back modules)"
    - "test_focus_loss_episode_boundary_parity passes when run after all 4 remaining tests in test_data_comparison.py (sequential within pytest invocation)"
    - "All 4 remaining tests in test_data_comparison.py still pass"
  artifacts:
    - path: "tests/e2e/test_focus_loss_data_parity.py"
      provides: "Isolated module for focus loss episode boundary parity test"
      contains: "test_focus_loss_episode_boundary_parity"
    - path: "tests/e2e/test_data_comparison.py"
      provides: "Data comparison tests without the focus loss boundary test"
      min_tests: 4
  key_links:
    - from: "tests/e2e/test_focus_loss_data_parity.py"
      to: "tests/conftest.py"
      via: "flask_server fixture (module-scoped, port 5702)"
      pattern: "flask_server"
    - from: "tests/e2e/test_focus_loss_data_parity.py"
      to: "tests/fixtures/network_helpers.py"
      via: "import set_tab_visibility, wait_for_focus_manager_state"
      pattern: "from tests\\.fixtures\\.network_helpers import"
    - from: "tests/e2e/test_focus_loss_data_parity.py"
      to: "tests/fixtures/game_helpers.py"
      via: "import game helpers"
      pattern: "from tests\\.fixtures\\.game_helpers import"
---

<objective>
Move `test_focus_loss_episode_boundary_parity` out of `test_data_comparison.py` into its own module so it gets a fresh module-scoped `flask_server` instance, eliminating the server state exhaustion failure that occurs when it runs as the 5th test on the same server.

Purpose: The verification of Phase 71-01 found that the module-scoped `flask_server` becomes unresponsive after serving 4 game sessions within `test_data_comparison.py`. Moving this test to its own module gives it a fresh server instance via the existing module-scoped fixture, which is the simplest and most reliable fix.

Output: New `tests/e2e/test_focus_loss_data_parity.py` module containing the moved test, and updated `tests/e2e/test_data_comparison.py` with the test removed.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/71-test-infrastructure-fix/71-01-SUMMARY.md
@.planning/phases/71-test-infrastructure-fix/71-VERIFICATION.md
@tests/e2e/test_data_comparison.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract test_focus_loss_episode_boundary_parity to its own module</name>
  <files>
    tests/e2e/test_focus_loss_data_parity.py
    tests/e2e/test_data_comparison.py
  </files>
  <action>
**Step 1: Create `tests/e2e/test_focus_loss_data_parity.py`**

Create a new test module with the following content:

1. Module docstring explaining this test was extracted from `test_data_comparison.py` to get a fresh `flask_server` instance (the module-scoped fixture exhausts eventlet/socketio state after 4 game sessions).

2. Imports -- copy exactly these from `test_data_comparison.py`:
```python
import os
import shutil
import time
import pytest

from tests.fixtures.export_helpers import (
    get_experiment_id,
    get_subject_ids_from_pages,
    wait_for_export_files,
    run_comparison,
)
from tests.fixtures.game_helpers import (
    wait_for_socket_connected,
    wait_for_game_canvas,
    wait_for_game_object,
    wait_for_episode_complete,
    get_game_state,
    click_advance_button,
    click_start_button,
    get_scene_id,
    run_full_episode_flow_until_gameplay,
)
from tests.fixtures.network_helpers import (
    set_tab_visibility,
    wait_for_focus_manager_state,
)
```

Note: `apply_latency` is NOT needed (not used by this test). `start_random_actions`/`stop_random_actions` from `input_helpers` are NOT needed either.

3. Copy the `clean_data_dir` fixture from `test_data_comparison.py` -- it is a function-scoped fixture defined locally in that file, so the new module needs its own copy. Keep it identical.

4. Copy `test_focus_loss_episode_boundary_parity` function exactly as-is from `test_data_comparison.py` (lines 504-613). Do NOT modify the test logic, fixtures, assertions, or timeouts. The function signature is:
```python
@pytest.mark.timeout(300)
def test_focus_loss_episode_boundary_parity(flask_server, player_contexts, clean_data_dir):
```

The test uses `flask_server` (module-scoped from conftest.py), `player_contexts` (function-scoped from conftest.py), and `clean_data_dir` (function-scoped, defined locally).

**Step 2: Remove test_focus_loss_episode_boundary_parity from `tests/e2e/test_data_comparison.py`**

Delete the entire `test_focus_loss_episode_boundary_parity` function (lines 504-613) from `test_data_comparison.py`. This includes the `@pytest.mark.timeout(300)` decorator and the entire function body.

Do NOT remove any imports from `test_data_comparison.py` -- the remaining tests still use `set_tab_visibility`, `wait_for_focus_manager_state`, `run_full_episode_flow_until_gameplay`, etc. (specifically, `test_focus_loss_mid_episode_parity` still needs them all).

Do NOT modify the module docstring of `test_data_comparison.py` -- the remaining 4 tests still validate data comparison.

Do NOT remove the `clean_data_dir` fixture from `test_data_comparison.py` -- the remaining tests still use it.

**What NOT to do:**
- Do NOT change fixture scopes
- Do NOT add any new fixtures
- Do NOT modify test logic
- Do NOT change port numbers
- Do NOT modify conftest.py
  </action>
  <verify>
1. Syntax check both files:
```bash
python -c "import ast; ast.parse(open('tests/e2e/test_focus_loss_data_parity.py').read()); print('New file: Syntax OK')"
python -c "import ast; ast.parse(open('tests/e2e/test_data_comparison.py').read()); print('Old file: Syntax OK')"
```

2. Verify the test exists in the new file:
```bash
grep -c 'def test_focus_loss_episode_boundary_parity' tests/e2e/test_focus_loss_data_parity.py
```
Expected: 1

3. Verify the test is removed from the old file:
```bash
grep -c 'def test_focus_loss_episode_boundary_parity' tests/e2e/test_data_comparison.py
```
Expected: 0

4. Verify test_data_comparison.py still has its 4 remaining tests:
```bash
grep -c 'def test_' tests/e2e/test_data_comparison.py
```
Expected: 4 (test_export_parity_basic, test_export_parity_with_latency, test_active_input_parity, test_focus_loss_mid_episode_parity)

5. Verify clean_data_dir fixture exists in both files:
```bash
grep -c 'def clean_data_dir' tests/e2e/test_focus_loss_data_parity.py
grep -c 'def clean_data_dir' tests/e2e/test_data_comparison.py
```
Expected: 1 and 1
  </verify>
  <done>
`test_focus_loss_episode_boundary_parity` exists in its own module `tests/e2e/test_focus_loss_data_parity.py` with all necessary imports and the `clean_data_dir` fixture. The test is removed from `test_data_comparison.py`. Both files pass syntax checks. `test_data_comparison.py` retains its 4 remaining tests.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify the extracted test passes after other test suites</name>
  <files>tests/e2e/test_focus_loss_data_parity.py</files>
  <action>
Run the extracted test back-to-back with `test_data_comparison.py` to prove:
1. The 4 remaining tests in `test_data_comparison.py` still pass
2. The extracted test passes when run immediately after (fresh flask_server for new module)

**Step 1:** Run `test_data_comparison.py` followed by `test_focus_loss_data_parity.py` in a single pytest invocation:
```bash
cd /Users/chasemcd/Repositories/interactive-gym
python -m pytest tests/e2e/test_data_comparison.py tests/e2e/test_focus_loss_data_parity.py --headed -x -v --timeout=300
```

The `-x` flag stops on first failure. This tests:
- All 4 data comparison tests pass on the module-scoped flask_server
- flask_server is torn down after test_data_comparison.py completes (port freed)
- A new flask_server starts for test_focus_loss_data_parity.py (fresh server state)
- test_focus_loss_episode_boundary_parity passes on the fresh server

**Step 2:** If Step 1 passes, verify no stale processes:
```bash
lsof -ti :5702 :5703 :5704 :5705 :5706 2>/dev/null || echo "All ports clean"
```

**If the test fails due to Page.goto timeout:** This would indicate the port teardown between modules is still broken (unlikely given 71-01 fix). Check `_teardown_server` logs.

**If the test fails due to test logic (not Page.goto timeout):** That is a pre-existing test logic issue, not an infrastructure issue. Document it but do not fix -- it is outside the scope of this gap closure.

**Timeout note:** Budget 20-25 minutes for the full verification run. Each test takes 1-3 minutes with headed Chromium.
  </action>
  <verify>
The pytest invocation `tests/e2e/test_data_comparison.py tests/e2e/test_focus_loss_data_parity.py` completes with all 5 tests passing (4 from data_comparison + 1 from focus_loss_data_parity). No Page.goto timeouts. `lsof` shows all ports clean.
  </verify>
  <done>
`test_focus_loss_episode_boundary_parity` passes when run after `test_data_comparison.py` (back-to-back modules). The server state exhaustion gap is closed -- the test gets a fresh `flask_server` instance in its own module.
  </done>
</task>

</tasks>

<verification>
Gap closure verification (from 71-VERIFICATION.md):

1. The partial truth "test_focus_loss_episode_boundary_parity passes when run after other test suites" is now fully satisfied:
   - The test runs in its own module with a fresh flask_server instance
   - Back-to-back execution (test_data_comparison -> test_focus_loss_data_parity) succeeds
   - Server state accumulation is eliminated because the module-scoped fixture creates a fresh server for each module

2. The remaining 4 tests in test_data_comparison.py continue to pass (no regression)

3. Requirements coverage:
   - INFRA-02 (Page.goto navigation succeeds consistently): The 5th test no longer runs on an exhausted server
   - REG-01 (All data comparison tests pass): 4 data comparison + 1 focus loss parity = all pass
   - REG-04 (All focus loss tests pass): focus_loss_episode_boundary_parity passes in its own module
</verification>

<success_criteria>
- `test_focus_loss_episode_boundary_parity` exists in `tests/e2e/test_focus_loss_data_parity.py`
- `test_focus_loss_episode_boundary_parity` is removed from `tests/e2e/test_data_comparison.py`
- `test_data_comparison.py` retains 4 tests (basic, latency, active input, mid-episode focus loss)
- Running `pytest tests/e2e/test_data_comparison.py tests/e2e/test_focus_loss_data_parity.py --headed -x` passes all 5 tests
- No Page.goto timeouts during back-to-back module execution
</success_criteria>

<output>
After completion, create `.planning/phases/71-test-infrastructure-fix/71-02-SUMMARY.md`
</output>

---
phase: 71-test-infrastructure-fix
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
autonomous: true

must_haves:
  truths:
    - "Running test_data_comparison then test_focus_loss (in test_data_comparison.py) back-to-back succeeds with no Page.goto timeouts"
    - "No stale server processes remain on ports 5702-5706 after any fixture teardown"
    - "All 5 server fixtures use identical robust teardown logic (DRY)"
    - "test_focus_loss_episode_boundary_parity passes when run after other test suites"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Robust server fixture lifecycle with port verification"
      contains: "_is_port_free"
    - path: "tests/conftest.py"
      provides: "Shared teardown helper"
      contains: "_teardown_server"
    - path: "tests/conftest.py"
      provides: "Pre-startup port cleanup"
      contains: "_ensure_port_available"
  key_links:
    - from: "flask_server fixture teardown"
      to: "_teardown_server helper"
      via: "function call in yield teardown"
      pattern: "_teardown_server\\(process, port"
    - from: "flask_server_fresh fixture teardown"
      to: "_teardown_server helper"
      via: "function call in yield teardown"
      pattern: "_teardown_server\\(process, port"
    - from: "flask_server fixture startup"
      to: "_ensure_port_available helper"
      via: "function call before Popen"
      pattern: "_ensure_port_available\\(port\\)"
    - from: "all Popen calls"
      to: "start_new_session=True"
      via: "Popen kwarg"
      pattern: "start_new_session=True"
---

<objective>
Add robust server process teardown with port-availability verification to all test fixtures, eliminating Page.goto timeout failures when running E2E test suites back-to-back.

Purpose: The `flask_server` fixture (port 5702) is shared by 5 test modules. When one module's server teardown races with the next module's startup, the new server cannot bind the port (TCP TIME_WAIT or eventlet greenlet leaks), causing Page.goto to timeout at 30s. This is the root cause of `test_focus_loss_episode_boundary_parity` failing when run after other suites.

Output: Updated `tests/conftest.py` with shared helper functions and all 5 server fixtures refactored to use them.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/71-test-infrastructure-fix/71-RESEARCH.md
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add shared teardown helpers and refactor all server fixtures</name>
  <files>tests/conftest.py</files>
  <action>
Add three helper functions at the top of `tests/conftest.py` (after imports, before fixtures) and refactor all 5 server fixtures to use them. Changes in detail:

**1. Add imports** (add to existing import block):
```python
import os
import signal
import socket
```

**2. Add `_is_port_free(port)` helper:**
- Uses `socket.socket(AF_INET, SOCK_STREAM)` with `SO_REUSEADDR`
- Tries `s.bind(("localhost", port))` -- returns True if succeeds, False on OSError
- This is the authoritative check (not connect-based, which can succeed during TIME_WAIT)

**3. Add `_ensure_port_available(port)` helper:**
- If `_is_port_free(port)` returns True, return immediately (common case)
- Otherwise, use `subprocess.run(["lsof", "-ti", f":{port}"])` to find stale PIDs
- Kill each PID with `os.kill(pid, signal.SIGKILL)`
- Wait 1 second, re-check with `_is_port_free(port)`
- If still occupied, raise `RuntimeError(f"Port {port} occupied by unkillable process")`
- Wrap lsof call in try/except for FileNotFoundError and subprocess.TimeoutExpired

**4. Add `_teardown_server(process, port, timeout_sec=10)` helper:**
- If `process.poll() is not None` (already dead), just call `_wait_for_port_free(port, timeout_sec)` and return
- Send `process.terminate()` (SIGTERM) wrapped in try/except OSError
- Wait with `process.wait(timeout=5)`
- On `TimeoutExpired`: call `process.kill()` then `process.wait(timeout=5)`
- Then try `os.killpg(os.getpgid(process.pid), signal.SIGKILL)` wrapped in try/except `(OSError, ProcessLookupError)`
- Call `_wait_for_port_free(port, timeout_sec)`

**5. Add `_wait_for_port_free(port, timeout_sec=10)` helper:**
- Poll `_is_port_free(port)` every 0.2s until deadline (`time.monotonic() + timeout_sec`)
- If timeout reached: call `_ensure_port_available(port)` as last resort (kills stale processes)
- Wait 0.5s, then re-check `_is_port_free(port)`
- If STILL occupied, raise `RuntimeError(f"Port {port} still in use after {timeout_sec}s teardown")`

**6. Refactor all 5 server fixture STARTUP sections** (flask_server, flask_server_fresh, flask_server_multi_episode, flask_server_multi_episode_fresh, flask_server_focus_timeout):
- Add `_ensure_port_available(port)` call BEFORE the `subprocess.Popen(...)` call
- Change `subprocess.Popen(...)` to include `start_new_session=True`
- Change `stdout=subprocess.PIPE` to `stdout=subprocess.DEVNULL` (prevents pipe buffer deadlock on long-running tests)
- Keep `stderr=subprocess.PIPE` (needed for crash diagnostics in the retry loop)
- NOTE for `flask_server_multi_episode` fixture: its teardown currently calls `process.communicate(timeout=5)` to read stdout for debug files at `/tmp/`. Since stdout is now DEVNULL, change the communicate() call in teardown to only read stderr. Write empty bytes to the stdout debug file, write stderr to the stderr debug file.

**7. Refactor all 5 server fixture TEARDOWN sections:**
- Replace ALL inline teardown code with a single call: `_teardown_server(process, port)`
- For `flask_server_multi_episode`: keep the stderr debug file writing BEFORE calling `_teardown_server`. Read stderr with `process.stderr.read()` before teardown kills the process.
- Remove the `time.sleep(3)` hacks in `flask_server_fresh` and `flask_server_multi_episode_fresh`

**8. Refactor the max_retries failure path** in each fixture:
- The `else` clause (after the for loop) currently calls `process.terminate()` + `process.wait(timeout=5)`. Replace with `_teardown_server(process, port)` for consistent cleanup.

**IMPORTANT - What NOT to change:**
- Do NOT change fixture scopes (module vs function)
- Do NOT change port numbers
- Do NOT change the health check polling logic (the `for attempt in range(max_retries)` loop)
- Do NOT change `player_contexts`, `multi_participant_contexts`, or `browser_type_launch_args` fixtures
- Do NOT change the crash detection logic inside the retry loop (`process.poll()` check)
- Keep all docstrings accurate (update if behavior changes)
  </action>
  <verify>
Run: `python -c "import ast; ast.parse(open('tests/conftest.py').read()); print('Syntax OK')"` to verify no syntax errors.

Then verify the key patterns exist:
- `grep -c '_teardown_server' tests/conftest.py` should return 6+ (1 definition + 5 fixture usages)
- `grep -c '_ensure_port_available' tests/conftest.py` should return 6+ (1 definition + 5 fixture usages)
- `grep -c 'start_new_session=True' tests/conftest.py` should return 5 (one per Popen call)
- `grep -c 'subprocess.DEVNULL' tests/conftest.py` should return 5 (one per Popen call)
- `grep -c 'time.sleep(3)' tests/conftest.py` should return 0 (removed)
- `grep -c 'subprocess.PIPE' tests/conftest.py` should return 5 (stderr only, one per fixture)
  </verify>
  <done>
All 5 server fixtures in `tests/conftest.py` use shared `_teardown_server()` and `_ensure_port_available()` helpers. Every `Popen` call uses `start_new_session=True` and `stdout=subprocess.DEVNULL`. No `time.sleep(3)` hacks remain. Port-availability polling replaces hope-based delays.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify back-to-back test suite execution</name>
  <files>tests/conftest.py</files>
  <action>
Run two test suites back-to-back that share port 5702 (the `flask_server` fixture) to prove the fix works. This is the specific failure scenario described in the phase goal.

**Step 1:** Run `test_data_comparison` then `test_focus_loss_episode_boundary_parity` in a single pytest invocation:
```bash
cd /Users/chasemcd/Repositories/interactive-gym
python -m pytest tests/e2e/test_data_comparison.py tests/e2e/test_multiplayer_basic.py --headed -x -v --timeout=300
```

This command runs test_data_comparison (5 tests on port 5702) followed by test_multiplayer_basic (which also uses flask_server on port 5702). The `-x` flag stops on first failure. If Page.goto times out, it will fail on the first test of the second module.

**Step 2:** If Step 1 passes, run the specific failing test after another suite:
```bash
python -m pytest tests/e2e/test_data_comparison.py::test_focus_loss_episode_boundary_parity --headed -v --timeout=300
```

This runs just the focus loss test (which uses flask_server) to confirm it passes in isolation as well.

**Step 3:** Verify no stale processes after test run:
```bash
lsof -ti :5702 :5703 :5704 :5705 :5706 2>/dev/null || echo "All ports clean"
```

**If tests fail due to reasons UNRELATED to Page.goto timeout** (e.g., test logic failures, WebRTC flakiness), that is acceptable for this phase -- Phase 71 only fixes infrastructure/teardown issues, not test logic. Document any non-infrastructure failures but do not fix them.

**If tests fail due to Page.goto timeout**, investigate:
1. Check if the port was actually freed (the `_wait_for_port_free` should have raised RuntimeError if not)
2. Check if `_ensure_port_available` was called before Popen
3. Add temporary debug logging to `_teardown_server` if needed

**Timeout note:** These tests run headed Chromium with real WebRTC, so they take 1-3 minutes per test. Budget 15-20 minutes for the full verification run. Use `--timeout=300` (5 min per test) to be generous.
  </action>
  <verify>
Both pytest invocations complete without Page.goto timeout errors. `lsof` confirms no stale processes on test ports after completion.
  </verify>
  <done>
Back-to-back test suites sharing port 5702 execute without Page.goto timeouts. No stale server processes remain after test completion. The infrastructure fix is validated.
  </done>
</task>

</tasks>

<verification>
Phase 71 verification criteria (from roadmap):

1. Running two test suites back-to-back (e.g., test_data_comparison then test_multiplayer_basic) succeeds with no Page.goto timeouts -- verified by Task 2 Step 1
2. No stale server processes remain after a test suite completes (verified by port check) -- verified by Task 2 Step 3
3. Browser contexts, server processes, and temporary files are fully cleaned up between tests -- verified by the _teardown_server helper's port polling and process group kill
4. test_focus_loss_episode_boundary_parity passes when run after other test suites -- verified by Task 2 Step 2

Requirements coverage:
- INFRA-01 (reliable startup/teardown): _ensure_port_available + _teardown_server + port polling
- INFRA-02 (no Page.goto timeouts): Port-free verification before startup prevents stale server interference
- INFRA-03 (proper cleanup): start_new_session + process group kill + port polling + stdout DEVNULL
</verification>

<success_criteria>
- All 5 server fixtures use shared _teardown_server() helper (DRY)
- All 5 server fixtures call _ensure_port_available() before subprocess.Popen (pre-cleanup)
- All 5 Popen calls use start_new_session=True and stdout=subprocess.DEVNULL
- No time.sleep(3) hacks remain in any fixture
- Back-to-back test suite execution succeeds without Page.goto timeouts
- No stale processes remain on ports 5702-5706 after test completion
</success_criteria>

<output>
After completion, create `.planning/phases/71-test-infrastructure-fix/71-01-SUMMARY.md`
</output>

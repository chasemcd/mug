---
phase: 71-test-audit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .planning/phases/71-test-audit/71-AUDIT.md
autonomous: true

must_haves:
  truths:
    - "Full pytest tests/e2e/ -v output has been captured to a file"
    - "Every failing test has a documented root cause"
    - "Each failure is categorized as test-infrastructure, fixture-issue, or production-bug"
    - "A categorized failure list exists that phases 72-73 can use as a work queue"
  artifacts:
    - path: ".planning/phases/71-test-audit/71-AUDIT.md"
      provides: "Categorized failure catalog with root causes"
      contains: "## Categorized Failures"
  key_links:
    - from: "71-AUDIT.md"
      to: "Phase 72 (test infra fixes)"
      via: "test-infrastructure category entries"
      pattern: "Category: test-infrastructure"
    - from: "71-AUDIT.md"
      to: "Phase 73 (production bug fixes)"
      via: "production-bug category entries"
      pattern: "Category: production-bug"
---

<objective>
Run the full E2E test suite (26 tests across 8 files), capture all output, and produce a categorized failure catalog with root causes for every failure.

Purpose: This audit is the foundation for v1.17 E2E Test Reliability. The v1.16 Pyodide Web Worker migration changed how all game operations run (moved to Worker thread with postMessage protocol). Three critical bugs were already fixed in Phase 70 (js.window in Worker, DataCloneError, action key types), but the full E2E suite has not been run since the migration. This phase discovers what is broken and WHY so phases 72-73 can fix things systematically.

Output: `.planning/phases/71-test-audit/71-AUDIT.md` containing full test results and categorized failure list.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/70-validation-and-cleanup/70-02-SUMMARY.md

Key codebase references (read as needed during analysis):
@tests/e2e/conftest.py
@tests/conftest.py
@tests/fixtures/game_helpers.py
@tests/fixtures/export_helpers.py
@tests/fixtures/network_helpers.py
@tests/fixtures/input_helpers.py
@tests/fixtures/multi_participant.py
@tests/e2e/test_infrastructure.py
@tests/e2e/test_multiplayer_basic.py
@tests/e2e/test_data_comparison.py
@tests/e2e/test_latency_injection.py
@tests/e2e/test_network_disruption.py
@tests/e2e/test_worker_validation.py
@tests/e2e/test_multi_participant.py
@tests/e2e/test_lifecycle_stress.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run full E2E suite and capture output</name>
  <files>.planning/phases/71-test-audit/71-raw-output.txt</files>
  <action>
Run the full E2E test suite with verbose output and capture everything:

```bash
cd /Users/chasemcd/Repositories/interactive-gym
python -m pytest tests/e2e/ -v --tb=long --timeout=300 2>&1 | tee .planning/phases/71-test-audit/71-raw-output.txt
```

IMPORTANT execution notes:
- Tests REQUIRE headed mode (conftest.py forces --headed). Playwright will launch visible Chromium.
- Tests start a Flask server subprocess on ports 5702-5706. Make sure nothing is listening on those ports.
- The full suite has 26 tests and could take 30-60 minutes with timeouts.
- Some tests use `flask_server_fresh` (per-function server) while others use `flask_server` (per-module). Tests within the same module share a server.
- If a test hangs at timeout (300s default), let it time out naturally -- the timeout output is diagnostic.
- If the ENTIRE suite cannot run (e.g., import error, fixture failure), capture that error and investigate the root cause. Common issues:
  - Port already in use (kill stale processes: `lsof -ti:5702 -ti:5703 -ti:5704 -ti:5705 -ti:5706 | xargs kill -9`)
  - Playwright browsers not installed (`python -m playwright install chromium`)
  - Missing Python dependencies

After the suite finishes, also run a quick collection to confirm all 26 tests were collected:
```bash
python -m pytest tests/e2e/ --collect-only -q 2>&1 >> .planning/phases/71-test-audit/71-raw-output.txt
```

If individual test files fail in ways that prevent other files from running, run each file separately:
```bash
for f in tests/e2e/test_infrastructure.py tests/e2e/test_multiplayer_basic.py tests/e2e/test_data_comparison.py tests/e2e/test_latency_injection.py tests/e2e/test_network_disruption.py tests/e2e/test_worker_validation.py tests/e2e/test_multi_participant.py tests/e2e/test_lifecycle_stress.py; do
  echo "========== $f ==========" >> .planning/phases/71-test-audit/71-raw-output.txt
  python -m pytest "$f" -v --tb=long --timeout=300 2>&1 >> .planning/phases/71-test-audit/71-raw-output.txt
done
```
  </action>
  <verify>
- File `.planning/phases/71-test-audit/71-raw-output.txt` exists and contains pytest output
- Output contains results for all 26 tests (or explains why some could not be collected)
- Each test shows PASSED, FAILED, ERROR, or TIMEOUT status
  </verify>
  <done>Raw test output captured for all 26 E2E tests with full tracebacks for failures</done>
</task>

<task type="auto">
  <name>Task 2: Analyze failures and produce categorized audit</name>
  <files>.planning/phases/71-test-audit/71-AUDIT.md</files>
  <action>
Read the raw output from Task 1 and produce a structured audit document.

For EVERY failing test, determine the root cause by:
1. Reading the traceback/error message
2. Reading the relevant test code to understand what was expected
3. Reading the relevant fixture/helper code to understand the mechanism
4. Reading the relevant production code if the failure suggests a production bug
5. Checking if the failure is related to v1.16 Worker migration (postMessage, serialization, Worker lifecycle, timing changes)

Categorize each failure into exactly ONE of these categories:

**test-infrastructure** -- Test fixtures, helpers, or configuration are broken/outdated. The production code works but the test code cannot exercise it properly.
Examples: wrong selectors, stale fixture assumptions, missing wait conditions, test server config issues.

**fixture-issue** -- Test fixture setup/teardown fails. The test never gets to exercise production code.
Examples: server startup failure, browser context creation failure, port conflicts, timeout in fixture.

**production-bug** -- Test correctly identifies broken production behavior. The test code is right, but the production code has a bug.
Examples: Worker postMessage serialization error, game state not exposed correctly, missing event handler.

Write `.planning/phases/71-test-audit/71-AUDIT.md` with this structure:

```markdown
# Phase 71: E2E Test Audit Results

## Summary

- **Total tests:** 26
- **Passed:** N
- **Failed:** N
- **Errors:** N
- **Date:** YYYY-MM-DD
- **Branch:** feature/waiting-room-overhaul
- **Commit:** (git rev-parse --short HEAD)

## Test Results

| # | Test | File | Status | Category | Root Cause (brief) |
|---|------|------|--------|----------|-------------------|
| 1 | test_server_starts_and_contexts_connect | test_infrastructure.py | PASSED/FAILED | - / category | - / brief |
| ... | ... | ... | ... | ... | ... |

## Categorized Failures

### test-infrastructure (N failures)

#### [Test Name]
- **File:** tests/e2e/test_xxx.py
- **Error:** (first line of error/traceback)
- **Root Cause:** (detailed explanation)
- **Fix Approach:** (what needs to change, which file(s))
- **Complexity:** simple / moderate / complex

### fixture-issue (N failures)

(same structure)

### production-bug (N failures)

(same structure)

## Passing Tests

(list of tests that passed, confirming they work post-Worker-migration)

## Phase 72 Work Queue (Test Infrastructure)

Ordered list of test-infrastructure and fixture-issue fixes, grouped by shared root cause where possible.

## Phase 73 Work Queue (Production Bugs)

Ordered list of production bug fixes, with affected tests for each bug.
```

IMPORTANT analysis guidelines:
- If a test times out waiting for `window.game` or `startButton`, check whether Worker initialization changes affected the timing. The v1.16 migration moved Pyodide to a Worker, which changes initialization sequence.
- If a test fails with JavaScript evaluation errors, check whether the game object properties changed (e.g., `game.pyodide` no longer exists, replaced by `game.worker`).
- If export/parity tests fail, check whether export file paths or formats changed.
- Group related failures. If 10 tests all fail because `wait_for_start_button_enabled` times out, that is ONE root cause with 10 affected tests.
- Be specific about fix approach. "Update the selector" is too vague. "Change `wait_for_game_object` to check `window.game.worker` instead of `window.game.pyodide`" is specific enough.
  </action>
  <verify>
- `.planning/phases/71-test-audit/71-AUDIT.md` exists
- Every failing test appears in exactly one category
- Every failing test has a root cause and fix approach
- Phase 72 and Phase 73 work queues are populated
- Summary numbers match the actual test output
  </verify>
  <done>Every test failure documented with root cause, category, and fix approach. Work queues ready for phases 72-73.</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. Count tests in audit: should total 26 (or document why fewer were collected)
2. PASSED + FAILED + ERROR = total collected
3. Every non-PASSED test has a category and root cause
4. Phase 72 work queue contains all test-infrastructure and fixture-issue items
5. Phase 73 work queue contains all production-bug items
6. No test appears in multiple categories
</verification>

<success_criteria>
- AUDIT-01: Full E2E suite executed with output captured (71-raw-output.txt)
- AUDIT-02: Every failure root-caused and categorized (71-AUDIT.md)
- Phase 72 and 73 have actionable work queues derived from the audit
</success_criteria>

<output>
After completion, create `.planning/phases/71-test-audit/71-01-SUMMARY.md`
</output>

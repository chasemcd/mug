---
phase: 74-stability-certification
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [".planning/phases/74-stability-certification/74-01-SUMMARY.md"]
autonomous: true

must_haves:
  truths:
    - "No xfail markers exist in any E2E test file"
    - "No tolerance hacks or known-flaky annotations exist in any E2E test file"
    - "The row_tolerance=15 in test_latency_injection.py is documented as justified (not a hack)"
  artifacts:
    - path: "tests/e2e/test_data_comparison.py"
      provides: "Data comparison tests with no xfail/skip markers"
    - path: "tests/e2e/test_focus_loss_data_parity.py"
      provides: "Focus loss parity test with no xfail/skip markers"
    - path: "tests/e2e/test_infrastructure.py"
      provides: "Infrastructure test with no xfail/skip markers"
    - path: "tests/e2e/test_latency_injection.py"
      provides: "Latency injection tests with no xfail/skip markers"
    - path: "tests/e2e/test_lifecycle_stress.py"
      provides: "Lifecycle stress tests with no xfail/skip markers"
    - path: "tests/e2e/test_multi_participant.py"
      provides: "Multi-participant tests with no xfail/skip markers"
    - path: "tests/e2e/test_multiplayer_basic.py"
      provides: "Multiplayer basic tests with no xfail/skip markers"
    - path: "tests/e2e/test_network_disruption.py"
      provides: "Network disruption tests with no xfail/skip markers"
  key_links: []
---

<objective>
Audit all 8 E2E test modules for STAB-02 compliance: verify no xfail markers, tolerance hacks, or known-flaky annotations exist anywhere in the test suite.

Purpose: STAB-02 requires the test suite to be clean of workarounds that mask real failures. This audit confirms the suite is honest before the 10-run stability certification begins.
Output: A documented audit confirming STAB-02 compliance or identifying any issues that must be resolved before Plan 02.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@tests/e2e/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scan all E2E test files for anti-patterns</name>
  <files>
    tests/e2e/test_data_comparison.py
    tests/e2e/test_focus_loss_data_parity.py
    tests/e2e/test_infrastructure.py
    tests/e2e/test_latency_injection.py
    tests/e2e/test_lifecycle_stress.py
    tests/e2e/test_multi_participant.py
    tests/e2e/test_multiplayer_basic.py
    tests/e2e/test_network_disruption.py
    tests/e2e/conftest.py
    tests/conftest.py
  </files>
  <action>
    Search ALL 8 E2E test modules plus both conftest.py files for the following anti-patterns:

    1. **xfail markers**: `pytest.mark.xfail`, `@pytest.mark.xfail`, `xfail(`
    2. **skip markers**: `pytest.mark.skip`, `@pytest.mark.skip`, `skipif`, `pytest.skip(`
    3. **Known-flaky annotations**: `flaky`, `known.flaky`, `intermittent`, `unstable` (in comments or decorators)
    4. **Tolerance hacks**: Any `tolerance`, `row_tolerance`, `TOLERANCE` usage -- for each one found, determine if it is a JUSTIFIED tolerance (with documented reasoning) or a hack (undocumented workaround to make tests pass)
    5. **Retry-and-hope patterns**: `retry`, `flaky`, `rerun`, `@pytest.mark.flaky`
    6. **TODO/FIXME/HACK/WORKAROUND**: These annotations in test code may indicate known issues

    For each finding, classify it as:
    - **VIOLATION**: Must be fixed before stability runs (blocks STAB-02)
    - **JUSTIFIED**: Documented engineering tolerance, not a hack (acceptable for STAB-02)
    - **CLEAN**: No issues found

    Specifically for `row_tolerance=15` in test_latency_injection.py (line 424): Verify the comment block (lines 418-423) documents WHY this tolerance exists (active input timing at episode boundaries under latency). This was already reviewed in Phase 73 and deemed justified -- confirm the documentation is still present and adequate.

    Record results in a structured format showing:
    - File scanned
    - Patterns searched
    - Findings (with line numbers)
    - Classification (VIOLATION / JUSTIFIED / CLEAN)
  </action>
  <verify>
    Run these grep commands and confirm zero results for violations:
    ```
    grep -rn "xfail\|pytest.mark.skip\|skipif\|flaky\|known.flaky\|HACK\|WORKAROUND" tests/e2e/
    ```
    The only matches should be:
    - `row_tolerance=15` in test_latency_injection.py (JUSTIFIED)
    - "timing hacks" string in test_multi_participant.py and test_lifecycle_stress.py (these say "should work WITHOUT timing hacks" -- a positive statement, not a hack annotation)
  </verify>
  <done>
    All 8 E2E test files plus 2 conftest.py files audited. Zero VIOLATION-class findings. STAB-02 is confirmed: no xfail markers, no tolerance hacks (only justified tolerances), no known-flaky annotations exist in the test suite.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify complete test inventory matches expectations</name>
  <files>
    tests/e2e/test_data_comparison.py
    tests/e2e/test_focus_loss_data_parity.py
    tests/e2e/test_infrastructure.py
    tests/e2e/test_latency_injection.py
    tests/e2e/test_lifecycle_stress.py
    tests/e2e/test_multi_participant.py
    tests/e2e/test_multiplayer_basic.py
    tests/e2e/test_network_disruption.py
  </files>
  <action>
    Run `python -m pytest tests/e2e/ --collect-only -q` and verify the output matches exactly 24 tests across 8 modules:

    Expected test inventory (24 tests):
    - test_data_comparison.py: 4 tests (test_export_parity_basic, test_export_parity_with_latency, test_active_input_parity, test_focus_loss_mid_episode_parity)
    - test_focus_loss_data_parity.py: 1 test (test_focus_loss_episode_boundary_parity)
    - test_infrastructure.py: 1 test (test_server_starts_and_contexts_connect)
    - test_latency_injection.py: 6 tests (2 parametrized x [200, 100] + 2 non-parametrized)
    - test_lifecycle_stress.py: 5 tests (multi_episode, mid_game_disconnect, waitroom_disconnect, focus_loss_timeout, mixed_lifecycle)
    - test_multi_participant.py: 2 tests (three_simultaneous, staggered_arrival)
    - test_multiplayer_basic.py: 2 tests (two_players_connect, matchmaking_pairs)
    - test_network_disruption.py: 3 tests (packet_loss, tab_visibility, active_input_packet_loss)

    If the count does NOT match 24, investigate which tests are missing or extra.

    Also verify:
    - All tests are parameterized with [chromium] only (no firefox, webkit)
    - The e2e conftest.py forces headed mode automatically
    - pytest.ini has `addopts = -v --tb=short`
  </action>
  <verify>
    `python -m pytest tests/e2e/ --collect-only -q 2>&1 | tail -1` should output "24 tests collected"
  </verify>
  <done>
    Exactly 24 tests confirmed across 8 modules. All parameterized with [chromium]. Headed mode auto-enabled via conftest.py. Test inventory is complete and ready for 10-run stability certification.
  </done>
</task>

</tasks>

<verification>
- Zero xfail markers across all E2E test files
- Zero skip markers (except any explicitly justified conditional skips)
- Zero tolerance hacks (row_tolerance=15 is justified with comments)
- Zero known-flaky annotations
- Exactly 24 tests collected across 8 modules
- STAB-02 requirement satisfied
</verification>

<success_criteria>
STAB-02 is confirmed clean. The test suite has no workarounds masking real failures. The exact test inventory (24 tests, 8 modules) is documented and ready for the 10-run stability certification in Plan 02.
</success_criteria>

<output>
After completion, create `.planning/phases/74-stability-certification/74-01-SUMMARY.md`
</output>

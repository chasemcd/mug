---
phase: 41-latency-injection
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/fixtures/network_helpers.py
  - tests/e2e/test_latency_injection.py
autonomous: true

must_haves:
  truths:
    - "Test can inject 100ms fixed latency and complete episode"
    - "Test can inject 200ms fixed latency and complete episode"
    - "Test can inject 500ms fixed latency and complete episode"
    - "Test can apply different latencies to each player (asymmetric)"
    - "Test can vary latency during gameplay (jitter)"
  artifacts:
    - path: "tests/fixtures/network_helpers.py"
      provides: "CDP latency injection utilities"
      exports: ["apply_latency", "JitterEmulator"]
    - path: "tests/e2e/test_latency_injection.py"
      provides: "Latency injection test suite"
      contains: "test_episode_completion_under_fixed_latency"
  key_links:
    - from: "tests/fixtures/network_helpers.py"
      to: "page.context.new_cdp_session"
      via: "Playwright CDP API"
      pattern: "new_cdp_session"
    - from: "tests/e2e/test_latency_injection.py"
      to: "tests/fixtures/network_helpers.py"
      via: "import"
      pattern: "from tests.fixtures.network_helpers import"
---

<objective>
Create CDP-based latency injection tests that validate data parity under various network conditions.

Purpose: Verify that the dual-buffer data recording (v1.8) produces identical exports under latency stress - the core assumption for research data validity.

Output: Network helper utilities and a test suite covering fixed latency, asymmetric latency, and jitter scenarios.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/41-latency-injection/41-RESEARCH.md

# Phase 40 infrastructure
@tests/conftest.py
@tests/fixtures/game_helpers.py
@tests/e2e/test_multiplayer_basic.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CDP latency injection helpers</name>
  <files>tests/fixtures/network_helpers.py</files>
  <action>
Create `tests/fixtures/network_helpers.py` with CDP network emulation utilities:

1. `apply_latency(page, latency_ms)` function:
   - Create CDP session via `page.context.new_cdp_session(page)`
   - Call `Network.enable` first (required before emulation)
   - Call `Network.emulateNetworkConditions` with:
     - `offline: False`
     - `latency: latency_ms`
     - `downloadThroughput: -1` (no limit)
     - `uploadThroughput: -1` (no limit)
   - Return the CDP session (caller may need to modify or detach)

2. `JitterEmulator` class (from research pattern):
   - `__init__(self, cdp_session, base_latency: int, jitter_range: int)`
   - `start(self, interval_ms: int = 100)` - start background thread
   - `stop()` - stop jitter emulation
   - Background thread randomly varies latency within `[base - range, base + range]`
   - Use daemon thread (automatically killed on test exit)
   - Clamp latency to minimum 0

3. Add docstrings explaining:
   - CDP only works with Chromium (tests must use chromium browser)
   - Network.enable is required before emulateNetworkConditions
   - -1 throughput means "no limit"
  </action>
  <verify>
File exists with both `apply_latency` function and `JitterEmulator` class:
```bash
grep -q "def apply_latency" tests/fixtures/network_helpers.py && \
grep -q "class JitterEmulator" tests/fixtures/network_helpers.py && \
echo "Helpers created"
```
  </verify>
  <done>
`apply_latency()` creates CDP session and applies fixed latency. `JitterEmulator` varies latency via background thread.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create latency injection test suite</name>
  <files>tests/e2e/test_latency_injection.py</files>
  <action>
Create `tests/e2e/test_latency_injection.py` with tests for NET-01, NET-04, NET-05:

**Test 1: Fixed latency (parametrized) - NET-01**
```python
@pytest.mark.parametrize("latency_ms", [100, 200, 500])
@pytest.mark.timeout(300)
def test_episode_completion_under_fixed_latency(flask_server, player_contexts, latency_ms):
    """Test that episode completes under fixed symmetric latency."""
```
- Apply same latency to both players
- Run full episode flow (same as test_two_players_connect_and_complete_episode)
- Assert episode completes for both players
- Log actual completion time and frame count

**Test 2: Asymmetric latency - NET-04**
```python
@pytest.mark.timeout(300)
def test_episode_completion_under_asymmetric_latency(flask_server, player_contexts):
    """Test that episode completes when players have different latencies."""
```
- Apply 100ms to player 1, 500ms to player 2
- Run full episode flow
- Assert episode completes for both players
- Verify they're in same gameId

**Test 3: Jitter - NET-05**
```python
@pytest.mark.timeout(300)
def test_episode_completion_under_jitter(flask_server, player_contexts):
    """Test that episode completes under variable latency (jitter)."""
```
- Apply jitter to both players (base=200ms, range=150ms -> 50-350ms)
- Start jitter before gameplay, stop after episode
- Run full episode flow
- Assert episode completes

**Test structure:**
- Reuse game flow from test_multiplayer_basic.py:
  1. Navigate to base_url
  2. Wait for socket connected
  3. Click advance button (instructions)
  4. Complete tutorial and advance
  5. Click start button (multiplayer)
  6. Wait for game canvas
  7. Wait for episode complete
- Apply latency AFTER navigation but BEFORE gameplay starts (CDP session created after page exists)

**Important:**
- Import helpers: `from tests.fixtures.network_helpers import apply_latency, JitterEmulator`
- Import game helpers: `from tests.fixtures.game_helpers import ...`
- Detach CDP sessions in test cleanup (or rely on context close)
- Stop JitterEmulator in finally block
  </action>
  <verify>
Run the test suite:
```bash
cd /Users/chasemcd/Repositories/interactive-gym && \
python -m pytest tests/e2e/test_latency_injection.py -v --timeout=600
```
All tests should pass (may take 10-15 minutes total due to episode timeouts).
  </verify>
  <done>
Three tests pass:
- `test_episode_completion_under_fixed_latency[100]`
- `test_episode_completion_under_fixed_latency[200]`
- `test_episode_completion_under_fixed_latency[500]`
- `test_episode_completion_under_asymmetric_latency`
- `test_episode_completion_under_jitter`
  </done>
</task>

</tasks>

<verification>
Run full test suite including new latency tests:
```bash
cd /Users/chasemcd/Repositories/interactive-gym && \
python -m pytest tests/e2e/ -v --timeout=600
```

Expected: All tests pass (infrastructure + latency injection tests).
</verification>

<success_criteria>
1. `tests/fixtures/network_helpers.py` exists with `apply_latency()` and `JitterEmulator`
2. `tests/e2e/test_latency_injection.py` exists with parametrized fixed latency test
3. Fixed latency tests pass for 100ms, 200ms, 500ms
4. Asymmetric latency test passes (100ms vs 500ms)
5. Jitter test passes (variable latency during gameplay)
6. All tests complete full episodes (verified by `num_episodes >= 1`)
</success_criteria>

<output>
After completion, create `.planning/phases/41-latency-injection/41-01-SUMMARY.md`
</output>

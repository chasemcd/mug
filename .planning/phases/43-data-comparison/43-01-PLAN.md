---
phase: 43-data-comparison
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/fixtures/export_helpers.py
  - tests/e2e/test_data_comparison.py
autonomous: true

must_haves:
  truths:
    - "Test collects export files from both players after episode ends"
    - "Test invokes validate_action_sequences.py --compare on exports"
    - "Test reports pass/fail based on comparison exit code"
    - "Failed comparisons produce actionable diagnostic output"
  artifacts:
    - path: "tests/fixtures/export_helpers.py"
      provides: "Export file collection and validation script invocation"
      exports: ["collect_export_files", "run_comparison", "wait_for_export_files"]
    - path: "tests/e2e/test_data_comparison.py"
      provides: "Data comparison test suite"
      contains: "test_export_parity"
  key_links:
    - from: "tests/e2e/test_data_comparison.py"
      to: "tests/fixtures/export_helpers.py"
      via: "import helpers"
      pattern: "from tests\\.fixtures\\.export_helpers import"
    - from: "tests/fixtures/export_helpers.py"
      to: "scripts/validate_action_sequences.py"
      via: "subprocess invocation"
      pattern: "subprocess\\.run.*validate_action_sequences"
---

<objective>
Create a data comparison pipeline that collects export files from both players after episode completion and validates parity using the existing `validate_action_sequences.py --compare` script.

Purpose: Enable automated validation that both players export identical game state data, proving v1.8 data export parity works in practice under controlled test conditions.

Output: Export collection helpers and a test that validates data parity between two players.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Prior work
@.planning/phases/40-test-infrastructure/40-01-SUMMARY.md
@.planning/phases/40-test-infrastructure/40-02-SUMMARY.md
@.planning/phases/42-network-disruption/42-01-SUMMARY.md

# Existing infrastructure
@tests/conftest.py
@tests/fixtures/game_helpers.py
@tests/fixtures/network_helpers.py
@scripts/validate_action_sequences.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create export collection helpers</name>
  <files>tests/fixtures/export_helpers.py</files>
  <action>
Create a new helper module for export file collection and comparison:

1. `collect_export_files(experiment_id: str, scene_id: str, subject_ids: tuple[str, str], episode_num: int) -> tuple[Path, Path]`:
   - Construct paths to export CSV files: `data/{experiment_id}/{scene_id}/{subject_id}_ep{episode_num}.csv`
   - Return tuple of (file1_path, file2_path)
   - Raise FileNotFoundError if files don't exist

2. `wait_for_export_files(experiment_id: str, scene_id: str, subject_ids: tuple[str, str], episode_num: int, timeout_sec: int = 30) -> tuple[Path, Path]`:
   - Poll for export files to exist (server writes them after episode ends)
   - Return paths once both files exist
   - Raise TimeoutError if files don't appear within timeout

3. `run_comparison(file1: Path, file2: Path, verbose: bool = False) -> tuple[int, str]`:
   - Invoke `python scripts/validate_action_sequences.py --compare {file1} {file2}` via subprocess
   - If verbose=True, add `--verbose` flag
   - Capture stdout/stderr
   - Return (exit_code, output_text)
   - Exit code 0 = files identical, 1 = divergences found

4. `get_subject_ids_from_pages(page1, page2) -> tuple[str, str]`:
   - Extract subject IDs from game objects via `window.game.subjectId` or `window.subjectId`
   - Return tuple of subject IDs for both players

5. `get_experiment_id() -> str`:
   - Return experiment ID from server config (default: "overcooked_human_human_multiplayer" based on example)
   - Or detect from existing data directory structure

Include docstrings explaining each function's purpose and parameters.
  </action>
  <verify>
File exists at tests/fixtures/export_helpers.py with all 5 functions defined. Run:
```
python -c "from tests.fixtures.export_helpers import collect_export_files, wait_for_export_files, run_comparison, get_subject_ids_from_pages, get_experiment_id; print('Imports OK')"
```
  </verify>
  <done>
- collect_export_files constructs correct paths for export CSVs
- wait_for_export_files polls with timeout until files exist
- run_comparison invokes validation script and returns exit code + output
- get_subject_ids_from_pages extracts IDs from Playwright pages
- get_experiment_id returns the experiment identifier
  </done>
</task>

<task type="auto">
  <name>Task 2: Create data comparison test</name>
  <files>tests/e2e/test_data_comparison.py</files>
  <action>
Create a test file that validates data parity between two players:

1. `test_export_parity_basic(flask_server, player_contexts)`:
   - Run a full episode flow (reuse pattern from test_multiplayer_basic.py)
   - Wait for episode to complete for both players
   - Extract subject IDs from game objects
   - Wait for export files to appear on disk
   - Invoke run_comparison() on the two export files
   - Assert exit code is 0 (files identical)
   - If assertion fails, print the comparison output for debugging

2. `test_export_parity_with_latency(flask_server, player_contexts)`:
   - Apply 100ms latency to player 2 (using apply_latency from network_helpers)
   - Run full episode flow
   - Wait for export files
   - Run comparison and assert parity
   - This validates that exports are identical even under latency conditions

3. Include a pytest fixture `clean_data_dir(flask_server)` that:
   - Clears the data directory before each test to avoid stale files
   - The data directory is `data/{experiment_id}/{scene_id}/`
   - Use shutil.rmtree to clean, then os.makedirs to recreate

Key patterns to follow from existing tests:
- Use @pytest.mark.timeout(300) for 5-minute timeout
- Import from tests.fixtures.export_helpers and tests.fixtures.game_helpers
- Reuse run_full_episode_flow from test_latency_injection.py
- Print diagnostic info when comparisons fail

Scene ID for the multiplayer scene: Inspect via the game object's `sceneId` property. The example uses scene names like "cramped_room" or similar - extract dynamically in the test.
  </action>
  <verify>
Run a quick syntax check:
```
python -m py_compile tests/e2e/test_data_comparison.py
```

Then verify test discovery:
```
pytest tests/e2e/test_data_comparison.py --collect-only
```
Should list at least 2 tests.
  </verify>
  <done>
- test_export_parity_basic validates parity after basic episode
- test_export_parity_with_latency validates parity under latency
- clean_data_dir fixture clears stale export files
- Failed comparisons print diagnostic output with divergence details
  </done>
</task>

<task type="auto">
  <name>Task 3: Add helper for scene ID extraction</name>
  <files>tests/fixtures/game_helpers.py</files>
  <action>
Add a helper function to extract the scene ID from a game page:

```python
def get_scene_id(page: Page) -> str:
    """Get the current scene ID from the game object."""
    return page.evaluate("() => window.game?.sceneId || null")
```

This is needed because the export path includes the scene_id, and we need to dynamically get it from the running game to find the export files.

Also add a helper to get subject ID:

```python
def get_subject_id(page: Page) -> str:
    """Get the subject ID for this player."""
    return page.evaluate("() => window.subjectId || window.game?.subjectId || null")
```

Add at the end of the existing file, preserving all existing helpers.
  </action>
  <verify>
```
python -c "from tests.fixtures.game_helpers import get_scene_id, get_subject_id; print('Imports OK')"
```
  </verify>
  <done>
- get_scene_id extracts sceneId from game object
- get_subject_id extracts subjectId from window or game object
- Both return None if not available (game not initialized)
  </done>
</task>

</tasks>

<verification>
1. Syntax check all new/modified files:
   ```
   python -m py_compile tests/fixtures/export_helpers.py
   python -m py_compile tests/e2e/test_data_comparison.py
   python -m py_compile tests/fixtures/game_helpers.py
   ```

2. Verify test discovery:
   ```
   pytest tests/e2e/test_data_comparison.py --collect-only
   ```

3. Verify imports work:
   ```
   python -c "from tests.fixtures.export_helpers import collect_export_files, run_comparison; print('export_helpers OK')"
   python -c "from tests.fixtures.game_helpers import get_scene_id, get_subject_id; print('game_helpers OK')"
   ```

Note: Full test execution depends on resolving the episode completion timeout issue noted in STATE.md. The tests are structurally correct and will pass once the underlying environment issue is fixed.
</verification>

<success_criteria>
- [x] tests/fixtures/export_helpers.py exists with collect_export_files, wait_for_export_files, run_comparison, get_subject_ids_from_pages, get_experiment_id
- [x] tests/e2e/test_data_comparison.py exists with test_export_parity_basic and test_export_parity_with_latency
- [x] tests/fixtures/game_helpers.py has get_scene_id and get_subject_id helpers
- [x] All files pass syntax check (py_compile)
- [x] pytest discovers tests correctly (--collect-only)
- [x] Comparison pipeline invokes validate_action_sequences.py --compare
- [x] Failed comparisons produce actionable diagnostic output
</success_criteria>

<output>
After completion, create `.planning/phases/43-data-comparison/43-01-SUMMARY.md`
</output>

---
phase: 73-network-regression-validation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: []
autonomous: true

must_haves:
  truths:
    - "All 5 latency injection tests pass in a single pytest run (100ms, 200ms, asymmetric, jitter, active input x2)"
    - "All 3 network disruption tests pass in a single pytest run (packet loss rollback, tab visibility fast-forward, active input + packet loss)"
  artifacts:
    - path: "tests/e2e/test_latency_injection.py"
      provides: "Latency injection test suite"
      contains: "test_episode_completion_under_fixed_latency"
    - path: "tests/e2e/test_network_disruption.py"
      provides: "Network disruption test suite"
      contains: "test_packet_loss_triggers_rollback"
  key_links:
    - from: "tests/e2e/test_network_disruption.py"
      to: "tests/e2e/test_latency_injection.py"
      via: "imports run_full_episode_flow"
      pattern: "from tests.e2e.test_latency_injection import run_full_episode_flow"
---

<objective>
Run all latency injection and network disruption E2E tests and ensure they pass. Diagnose and fix any failures inline.

Purpose: Validate NET-01 (latency injection) and NET-02 (network disruption) requirements. Network disruption tests were never validated during Phase 70 or Phase 72 -- this is their first full run since Phase 71/72 infrastructure fixes.

Output: All 8 network-condition tests passing in a single run. Any code fixes needed will be committed. A SUMMARY documenting pass/fail results and any fixes applied.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@tests/e2e/test_latency_injection.py
@tests/e2e/test_network_disruption.py
@tests/conftest.py
@tests/fixtures/network_helpers.py
@tests/fixtures/game_helpers.py
@tests/fixtures/input_helpers.py
@tests/fixtures/export_helpers.py
@interactive_gym/server/static/js/pyodide_multiplayer_game.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Run latency injection test suite and fix any failures</name>
  <files>tests/e2e/test_latency_injection.py</files>
  <action>
Run the full latency injection test suite:

```bash
cd /Users/chasemcd/Repositories/interactive-gym
python -m pytest tests/e2e/test_latency_injection.py --headed --browser chromium -v 2>&1
```

This suite has 5 tests (parametrized):
- test_episode_completion_under_fixed_latency[chromium-100] (NET-01)
- test_episode_completion_under_fixed_latency[chromium-200] (NET-01, fixed in Phase 72)
- test_episode_completion_under_asymmetric_latency (NET-04)
- test_episode_completion_under_jitter (NET-05)
- test_active_input_with_latency[chromium-100] (INPUT-04)
- test_active_input_with_latency[chromium-200] (INPUT-04)

Wait -- that is 6 tests (2 parametrized x 2 + 2 non-parametrized). Count the actual output.

Expected: All pass. The 200ms latency test was fixed in Phase 72 (P2P ready gate timeout 5000ms -> 15000ms).

IF ANY TEST FAILS:
1. Capture the full error output including any traceback
2. Read the relevant test code and fixture code to understand the failure
3. Check browser console logs if available (some tests print them)
4. Diagnose root cause -- is it a timeout? assertion error? infrastructure issue?
5. Apply minimal fix (prefer timeout adjustments or test config changes over production code changes)
6. Re-run the failing test to verify the fix
7. Re-run the full suite to confirm no regressions

IMPORTANT: Each test takes 2-5 minutes. The full suite may take 15-30 minutes. Use `--timeout=300` (already set per-test via markers).

Record the complete pass/fail results for the SUMMARY.
  </action>
  <verify>
All tests in `python -m pytest tests/e2e/test_latency_injection.py --headed --browser chromium -v` show PASSED status with 0 failures.
  </verify>
  <done>Every latency injection test passes in a single run. Results documented.</done>
</task>

<task type="auto">
  <name>Task 2: Run network disruption test suite and fix any failures</name>
  <files>tests/e2e/test_network_disruption.py</files>
  <action>
Run the full network disruption test suite:

```bash
cd /Users/chasemcd/Repositories/interactive-gym
python -m pytest tests/e2e/test_network_disruption.py --headed --browser chromium -v 2>&1
```

This suite has 3 tests:
- test_packet_loss_triggers_rollback (NET-02: 15% packet loss + active inputs)
- test_tab_visibility_triggers_fast_forward (NET-03: simulated focus loss/refocus)
- test_active_input_with_packet_loss (INPUT-05: packet loss + active inputs + data parity)

These tests were NEVER VALIDATED after Phase 71/72 infrastructure fixes. They are the highest-risk tests in this phase.

Key risk areas:
- test_packet_loss_triggers_rollback imports `run_full_episode_flow` from test_latency_injection.py
- test_tab_visibility_triggers_fast_forward manually manages visibility and checks fast-forward state
- test_active_input_with_packet_loss combines packet loss + active inputs + export parity check

IF ANY TEST FAILS:
1. Capture the full error output
2. For packet loss tests: check if rollbacks actually occurred (stats logging in test output)
3. For tab visibility test: check if fast-forward state reporting works (`get_fast_forward_state`)
4. For parity tests: check if export files exist and what the comparison output shows
5. Diagnose root cause and apply minimal fix
6. Re-run failing test, then full suite

Common failure modes to watch for:
- Timeout waiting for episode complete (may need timeout increase)
- Assertion on rollback count = 0 (packet loss may not trigger rollbacks if timing is lucky)
- Fast-forward frame jump too small (timing-dependent)
- Export file not found (server may not write exports under disruption)

Record the complete pass/fail results for the SUMMARY.
  </action>
  <verify>
All tests in `python -m pytest tests/e2e/test_network_disruption.py --headed --browser chromium -v` show PASSED status with 0 failures.
  </verify>
  <done>Every network disruption test passes in a single run. Results documented. NET-01 and NET-02 requirements satisfied.</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. Run both suites together to confirm they work in sequence:
   ```bash
   python -m pytest tests/e2e/test_latency_injection.py tests/e2e/test_network_disruption.py --headed --browser chromium -v
   ```
2. Confirm 0 failures, 0 errors across all tests
3. Document any fixes applied and their rationale
</verification>

<success_criteria>
- All latency injection tests pass (100ms, 200ms, asymmetric, jitter, active input variants)
- All network disruption tests pass (packet loss, tab visibility, active input + packet loss)
- No xfail markers, tolerance hacks, or skip annotations added
- Any code fixes are minimal and well-documented
- NET-01 and NET-02 requirements marked as satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/73-network-regression-validation/73-01-SUMMARY.md`
</output>

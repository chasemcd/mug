---
phase: 72-latency-test-diagnosis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/e2e/test_latency_injection.py
autonomous: true

must_haves:
  truths:
    - "The exact stall point (setup vs gameplay) of the 200ms latency test is identified via timing output"
    - "Browser console output reveals P2P validation state (success, failure, timeout, re-pool)"
    - "Root cause is documented with evidence from the diagnostic run"
  artifacts:
    - path: "tests/e2e/test_latency_injection.py"
      provides: "Diagnostic instrumentation for timing and console capture"
      contains: "DIAG"
  key_links:
    - from: "tests/e2e/test_latency_injection.py"
      to: "stdout"
      via: "print statements with timing data and console log capture"
      pattern: "\\[DIAG\\]"
---

<objective>
Diagnose WHY `test_episode_completion_under_fixed_latency[chromium-200]` times out at 300s by adding timing instrumentation and browser console capture to identify the exact stall point.

Purpose: The root cause is unknown. Research (72-RESEARCH.md) identifies 5 hypotheses ranked by likelihood. The #1 hypothesis (HIGH) is that the P2P ready gate timeout (5000ms) is insufficient under 200ms symmetric SocketIO latency, causing validation failure, re-pooling, and potentially an infinite re-pool loop. We need empirical evidence to confirm or refute this.

Output: A diagnostic run that produces timing data and console output showing exactly where the test stalls, plus root cause documentation in the SUMMARY.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/72-latency-test-diagnosis/72-RESEARCH.md

Key source files:
@tests/e2e/test_latency_injection.py
@tests/fixtures/network_helpers.py
@tests/fixtures/game_helpers.py
@interactive_gym/server/static/js/pyodide_multiplayer_game.js (P2P ready gate at line 1130-1135, validation at 1137-1142, gate timeout at 1342-1361)
@interactive_gym/server/static/js/index.js (re-pool handler at line 963-982)
@interactive_gym/server/app.py (p2p_validation_failed handler)

Research hypothesis rankings:
1. HIGH: P2P ready gate race (5s timeout insufficient under 200ms symmetric latency)
2. MEDIUM: SocketIO fallback + rollback cascade
3. MEDIUM: Re-pool infinite loop (only 2 players, deterministic timing)
4. LOW: Pyodide CDN loading race
5. MEDIUM: Game loop never starts (timer worker not initialized)

Key insight from research: CDP `Network.emulateNetworkConditions` latency does NOT affect WebRTC DataChannel -- only HTTP/WebSocket. So P2P validation ping/pong on DataChannel is fast, but the server's `p2p_validation_complete` confirmation travels through latency-delayed SocketIO.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add diagnostic instrumentation to 200ms latency test</name>
  <files>tests/e2e/test_latency_injection.py</files>
  <action>
Add a new diagnostic wrapper function `run_full_episode_flow_with_diagnostics` that replaces `run_full_episode_flow` for the 200ms test case. This function should:

1. **Console capture**: Before navigation, attach console log handlers to both pages:
   ```python
   console_logs = {"p1": [], "p2": []}
   page1.on("console", lambda msg: console_logs["p1"].append(f"[{time.monotonic()-t0:.1f}s] {msg.text}"))
   page2.on("console", lambda msg: console_logs["p2"].append(f"[{time.monotonic()-t0:.1f}s] {msg.text}"))
   ```

2. **Timing instrumentation**: Wrap each step of `run_full_episode_flow` with `time.monotonic()` timing and print `[DIAG]` prefixed messages:
   - `[DIAG] Navigation complete: {elapsed}s`
   - `[DIAG] Socket connected: {elapsed}s`
   - `[DIAG] Advance button clicked: {elapsed}s`
   - `[DIAG] Start button clicked: {elapsed}s`
   - `[DIAG] Game canvas visible: {elapsed}s`
   - `[DIAG] Game object ready: {elapsed}s`
   - `[DIAG] Tab visibility set: {elapsed}s`
   - `[DIAG] Game state verified: {elapsed}s`
   - `[DIAG] Episode complete: {elapsed}s`

3. **P2P state capture**: After `wait_for_game_object` succeeds, evaluate and print P2P metrics:
   ```python
   p2p_state = page.evaluate("""() => ({
       p2pConnected: window.game?.p2pConnected,
       validationState: window.game?.p2pValidation?.state,
       gateResolved: window.game?.p2pReadyGate?.resolved,
       fallbackTriggered: window.game?.p2pMetrics?.p2pFallbackTriggered,
       timerWorkerActive: !!window.game?.timerWorker
   })""")
   ```

4. **Periodic game state polling**: After setting tab visibility, start a background thread that polls game state every 5 seconds and prints it, so if the episode stalls we can see frame progression:
   ```python
   def poll_game_state():
       while not done_event.is_set():
           try:
               state = page1.evaluate("() => ({ frame: window.game?.frameNumber, step: window.game?.step_num, episodes: window.game?.num_episodes, rollbacks: window.game?.rollbackCount })")
               print(f"[DIAG] Game state poll: {state}")
           except: pass
           done_event.wait(5.0)
   ```
   NOTE: Playwright pages are NOT thread-safe. Do NOT use a background thread to call page.evaluate(). Instead, use the main thread to poll game state. Since wait_for_episode_complete uses page.wait_for_function (which blocks), the polling approach must be different. The simplest approach: instead of using wait_for_episode_complete, write a manual polling loop that checks episode completion every 5 seconds and prints diagnostic state each iteration:
   ```python
   import time
   deadline = time.monotonic() + 180  # 180s episode timeout
   while time.monotonic() < deadline:
       state = page1.evaluate("() => ({ frame: window.game?.frameNumber, step: window.game?.step_num, episodes: window.game?.num_episodes, rollbacks: window.game?.rollbackCount, confirmed: window.game?.confirmedFrame })")
       print(f"[DIAG] Poll: {state}")
       if state and state.get("episodes", 0) >= 1:
           break
       time.sleep(5.0)
   else:
       # Dump all console logs on timeout
       print("\n[DIAG] === TIMEOUT - Console logs P1 ===")
       for log in console_logs["p1"][-50:]:
           print(f"  {log}")
       print("\n[DIAG] === TIMEOUT - Console logs P2 ===")
       for log in console_logs["p2"][-50:]:
           print(f"  {log}")
       pytest.fail("Episode did not complete within 180s")
   ```

5. **Console log dump on completion or failure**: Print the last 30 console log lines for each player at the end of the test (success or failure).

6. **Modify test_episode_completion_under_fixed_latency**: For the 200ms case specifically, use the diagnostic flow. Keep the 100ms case using the existing flow unchanged. The simplest approach: in run_full_episode_flow, add an optional `diagnostics=False` parameter. When called with `diagnostics=True`, enable all the instrumentation above. In the test function, pass `diagnostics=(latency_ms == 200)`.

Import `time` at the top of the file if not already imported.

Do NOT use background threads for Playwright page interaction. All page.evaluate calls must happen on the main thread.
  </action>
  <verify>
Run `python -m pytest tests/e2e/test_latency_injection.py::test_episode_completion_under_fixed_latency -k "200" --headed --timeout=300 -s 2>&1 | head -200` and confirm:
1. `[DIAG]` timing lines appear in output
2. Console log lines appear (at least `[P2P]` messages)
3. The test either completes (with timing data) or times out (with console dump showing the stall point)
  </verify>
  <done>
The diagnostic run produces clear evidence of WHERE the test stalls:
- If stalls during setup: timing shows which step (navigation, socket, canvas, etc.) and console shows P2P validation state
- If stalls during gameplay: polling shows frame progression (or lack thereof) and rollback counts
Root cause is identified from the evidence and printed in the test output.
  </done>
</task>

<task type="auto">
  <name>Task 2: Document root cause from diagnostic output</name>
  <files>tests/e2e/test_latency_injection.py</files>
  <action>
After the diagnostic test run completes (success or timeout), analyze the output to determine the root cause. Based on the research hypotheses, look for these signals in the output:

**Hypothesis #1 confirmed (P2P gate race):** Timing shows `wait_for_game_canvas` taking a long time or timing out. Console logs show "P2P connection timeout" or "validation not complete" followed by "Re-pool requested". Multiple "pyodide_game_ready" events appear (re-pool loop).

**Hypothesis #2 confirmed (SocketIO fallback):** Game starts but polling shows frames advancing very slowly. Console shows heavy rollback activity. `p2pFallbackTriggered` is true.

**Hypothesis #3 confirmed (re-pool loop):** Multiple rounds of re-pooling visible in console logs. Never reaches gameplay.

**Hypothesis #4 confirmed (Pyodide race):** Start button stays disabled for a long time. Timing shows click_start_button taking unusually long.

**Hypothesis #5 confirmed (game loop never starts):** Game object exists but frame number never advances from 0. Timer worker not active.

Based on the evidence, add a comment block at the top of the test file documenting:
```python
# ROOT CAUSE (Phase 72 diagnosis):
# [Description of what was found]
# [Which hypothesis was confirmed]
# [Evidence from diagnostic output]
```

If the diagnostic instrumentation is no longer needed after root cause is identified (the fix will be in Plan 02), you may clean it up or leave it -- the priority is documenting the root cause clearly.

NOTE: The root cause finding should also be captured in the SUMMARY for this plan.
  </action>
  <verify>
The root cause is identified and documented. At minimum:
1. A clear statement of what causes the timeout (e.g., "P2P ready gate 5000ms timeout races against SocketIO-delayed validation confirmation under 200ms latency")
2. Evidence supporting this conclusion (timing data, console log patterns)
3. The fix direction is clear (e.g., "Increase P2P ready gate timeout" or "Redesign test to avoid latency on signaling path")
  </verify>
  <done>
PERF-01 is satisfied: Root cause of `test_episode_completion_under_fixed_latency[chromium-200]` timeout is documented with evidence.
  </done>
</task>

</tasks>

<verification>
- [DIAG] timing output shows which step of the test flow consumes the most time
- Console log capture shows P2P validation state transitions
- Root cause is documented in the test file and plan SUMMARY
- The 100ms latency test was not modified (no regression risk)
</verification>

<success_criteria>
1. Root cause of 200ms latency test timeout is empirically identified (not just hypothesized)
2. Evidence from diagnostic output supports the conclusion
3. Fix direction is clear for Plan 02
4. PERF-01 requirement satisfied
</success_criteria>

<output>
After completion, create `.planning/phases/72-latency-test-diagnosis/72-01-SUMMARY.md`
</output>

---
phase: 69-multiplayer-batch-operations
plan: 03
type: execute
wave: 3
depends_on: ["69-02"]
files_modified:
  - interactive_gym/server/static/js/pyodide_multiplayer_game.js
  - interactive_gym/server/static/js/PyodideWorker.js
  - interactive_gym/server/static/js/pyodide_worker.js
  - interactive_gym/server/static/js/pyodide_remote_game.js
autonomous: true

must_haves:
  truths:
    - "performRollback() uses this.worker.batch() for setState + N replay steps + getState in a single Worker round-trip"
    - "_performFastForward() uses this.worker.batch() for N steps in a single Worker round-trip"
    - "Zero this.pyodide references remain in pyodide_multiplayer_game.js"
    - "runPythonAsync, toPy, _wrapResult shims removed from PyodideWorker.js"
    - "handleRunPython removed from pyodide_worker.js"
    - "this.pyodide = this.worker shim removed from pyodide_remote_game.js"
    - "Rollback replay data (per-frame rewards, actions, terminateds, truncateds, infos) is still captured and stored in frameDataBuffer"
  artifacts:
    - path: "interactive_gym/server/static/js/pyodide_multiplayer_game.js"
      provides: "Multiplayer game fully migrated to Worker commands including batch operations"
      contains: "this\\.worker\\.batch\\("
    - path: "interactive_gym/server/static/js/PyodideWorker.js"
      provides: "Clean Worker wrapper with no backward-compat shims"
    - path: "interactive_gym/server/static/js/pyodide_worker.js"
      provides: "Worker script with no handleRunPython"
    - path: "interactive_gym/server/static/js/pyodide_remote_game.js"
      provides: "Base class with no this.pyodide = this.worker shim"
  key_links:
    - from: "interactive_gym/server/static/js/pyodide_multiplayer_game.js"
      to: "interactive_gym/server/static/js/PyodideWorker.js"
      via: "this.worker.batch()"
      pattern: "this\\.worker\\.batch\\("
---

<objective>
Migrate batch operations (performRollback, _performFastForward) to use the Worker batch API, then remove all backward-compatibility shims.

Purpose: The batch API is the critical deliverable of Phase 69 -- it reduces rollback latency by executing setState + N steps + getState in a single Worker postMessage round-trip instead of multiple round-trips. After migration, all shims from Phase 68 are removed since no code uses runPythonAsync/toPy anymore.

Output: Fully migrated multiplayer game with batch rollback/fast-forward, and clean Worker files with no backward-compat shims.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/69-multiplayer-batch-operations/69-RESEARCH.md
@.planning/phases/69-multiplayer-batch-operations/69-02-SUMMARY.md
@interactive_gym/server/static/js/pyodide_multiplayer_game.js
@interactive_gym/server/static/js/PyodideWorker.js
@interactive_gym/server/static/js/pyodide_worker.js
@interactive_gym/server/static/js/pyodide_remote_game.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Migrate performRollback to use worker.batch()</name>
  <files>interactive_gym/server/static/js/pyodide_multiplayer_game.js</files>
  <action>
Replace the single massive runPythonAsync call in performRollback() (~line 4827) with a this.worker.batch() call.

**Current flow (single runPythonAsync):**
The existing code builds a giant Python script that:
1. Iterates over replay frames
2. For each frame: get_state (for snapshot), step, collect per-frame data (rewards, terminateds, etc.)
3. After all frames: get final state and hash
4. Returns JSON with replay_log, snapshots, replay_rewards, etc.

**New flow (Worker batch):**
Build an array of batch operations and process results on the JS side.

**Implementation:**

Replace the section from `// Execute ALL replay steps in a single Python call` through the end of the `if (replayFrames.length > 0)` block (~lines 4820-4944) with:

```javascript
// Execute ALL replay steps in a single Worker batch (single round-trip)
if (replayFrames.length > 0) {
    // Log replay actions for debugging
    const replayActionsStr = replayFrames.map(rf => `${rf.frame}:{${Object.entries(rf.actions).map(([k,v]) => `${k}:${v}`).join(',')}}`).join(' ');
    p2pLog.debug(`REPLAY: snapshotFrame=${snapshotFrame} frames=[${replayActionsStr}]`);

    // Build batch operations
    const batchOps = [];

    for (const rf of replayFrames) {
        // Save snapshot BEFORE stepping if this frame is on snapshot interval
        if (rf.frame % this.snapshotInterval === 0) {
            batchOps.push({ op: 'getState', params: {} });
        }

        // Compute pre-step hash for debug logging
        batchOps.push({ op: 'computeHash', params: {} });

        // Step with this frame's actions
        batchOps.push({ op: 'step', params: { actions: rf.actions } });

        // Compute post-step hash for debug logging
        batchOps.push({ op: 'computeHash', params: {} });
    }

    // Get final state after all replays (for snapshot update)
    batchOps.push({ op: 'getState', params: {} });

    // Execute entire batch in single Worker round-trip
    const batchResults = await this.worker.batch(batchOps);

    // Process batch results
    // Each replay frame produces 3-4 results depending on snapshot interval:
    // [getState?], computeHash, step, computeHash
    let resultIdx = 0;
    const replayLog = [];
    const snapshotsToSave = {};
    const replayCumulativeRewards = {};

    for (const rf of replayFrames) {
        // Check for snapshot getState
        let snapshotState = null;
        if (rf.frame % this.snapshotInterval === 0) {
            const stateJson = batchResults[resultIdx++];
            snapshotState = JSON.parse(stateJson);
            snapshotsToSave[rf.frame] = snapshotState;
        }

        // Pre-step hash
        const preHash = batchResults[resultIdx++].hash;

        // Step result
        const stepResult = batchResults[resultIdx++];
        let { obs, rewards, terminateds, truncateds, infos } = stepResult;

        // Convert to plain objects for logging (step returns plain objects from Worker)
        const rewardsDict = rewards instanceof Map ? Object.fromEntries(rewards) : (typeof rewards === 'object' ? rewards : { human: rewards });
        const termDict = terminateds instanceof Map ? Object.fromEntries(terminateds) : (typeof terminateds === 'object' ? terminateds : { human: terminateds });
        const truncDict = truncateds instanceof Map ? Object.fromEntries(truncateds) : (typeof truncateds === 'object' ? truncateds : { human: truncateds });
        const infosDict = infos instanceof Map ? Object.fromEntries(infos) : (typeof infos === 'object' ? infos : { human: infos });

        // Accumulate rewards
        for (const [agentId, reward] of Object.entries(rewardsDict)) {
            replayCumulativeRewards[agentId] = (replayCumulativeRewards[agentId] || 0) + reward;
        }

        // Post-step hash
        const postHash = batchResults[resultIdx++].hash;

        // Build replay log entry
        replayLog.push({
            frame: rf.frame,
            actions: rf.actions,
            rewards: rewardsDict,
            terminateds: termDict,
            truncateds: truncDict,
            infos: infosDict,
            pre_hash: preHash,
            post_hash: postHash
        });
    }

    // Final getState for post-replay snapshot
    const finalStateJson = batchResults[resultIdx++];

    // Log each replay frame for comparison
    for (const entry of replayLog) {
        const actionsStr = Object.entries(entry.actions).map(([k,v]) => `${k}:${v}`).join(',');
        p2pLog.debug(`REPLAY_FRAME: ${entry.frame} pre_hash=${entry.pre_hash} actions={${actionsStr}} post_hash=${entry.post_hash}`);
    }
    p2pLog.debug(`REPLAY_DONE: ${replayLog.length} frames replayed, final_state captured`);

    // Update snapshots with corrected state from replay
    for (const [frameStr, snapshotData] of Object.entries(snapshotsToSave)) {
        const frame = parseInt(frameStr);
        snapshotData.cumulative_rewards = {...this.cumulative_rewards};
        snapshotData.step_num = this.step_num;
        this.stateSnapshots.set(frame, JSON.stringify(snapshotData));
        p2pLog.debug(`SNAPSHOT_UPDATED: frame=${frame} (corrected after rollback)`);
    }

    // Add rewards accumulated during replay
    if (Object.keys(replayCumulativeRewards).length > 0) {
        for (const [playerId, reward] of Object.entries(replayCumulativeRewards)) {
            if (this.cumulative_rewards[playerId] !== undefined) {
                this.cumulative_rewards[playerId] += reward;
            } else {
                this.cumulative_rewards[playerId] = reward;
            }
        }
        p2pLog.debug(`REPLAY_REWARDS: added ${JSON.stringify(replayCumulativeRewards)} -> total=${JSON.stringify(this.cumulative_rewards)}`);
        ui_utils.updateHUDText(this.getHUDText());
    }

    // Update step_num
    this.step_num += replayLog.length;

    // Store corrected frame data in the rollback-safe buffer
    for (const entry of replayLog) {
        this.storeFrameData(entry.frame, {
            actions: entry.actions,
            rewards: entry.rewards,
            terminateds: entry.terminateds,
            truncateds: entry.truncateds,
            infos: entry.infos,
            isFocused: this.getFocusStatePerPlayer()
        });
    }
    p2pLog.debug(`Stored ${replayLog.length} corrected frames in data buffer`);
}
```

**CRITICAL CORRECTNESS CONCERNS:**
1. The Worker's handleStep normalizes rewards to dicts. The batch step results will have plain object rewards (not Python dicts with .items()). The JS-side processing above handles this.
2. The original code captured snapshots with RNG state during replay. The new code calls getState which captures full state including RNG. This is correct.
3. The computeHash calls in the batch are for debug logging only. If they add too much overhead, they can be conditionally included (e.g., only for first 100 frames). For now, include them to maintain parity with original behavior.
4. The step results from the Worker include render_state, but we don't need it during replay (we only render after the final frame). The render_state is ignored in the replay loop.

**Also update the post-rollback render code** (already migrated in Plan 02 to `this.worker.render()`) -- just confirm it still works correctly after the batch migration.

**Keep all existing code OUTSIDE the replay block unchanged:**
- Rollback guard (rollbackInProgress flag)
- Snapshot finding (findBestSnapshot)
- loadStateSnapshot call (already migrated in Plan 02)
- Action sequence clearing and rebuilding
- Frame data clearing (clearFrameDataFromRollback)
- JS frame counter update
- The finally block clearing rollbackInProgress
  </action>
  <verify>
1. Read performRollback() and confirm:
   - Uses this.worker.batch() for the replay
   - No this.pyodide references
   - replayLog still captures per-frame rewards/terminateds/truncateds/infos
   - Snapshots still saved at snapshotInterval
   - cumulative_rewards updated from replay
   - storeFrameData called for each replay frame
2. Grep for `this.pyodide` in performRollback -- should be zero
3. Grep for `worker.batch` in performRollback -- should appear once
  </verify>
  <done>performRollback uses this.worker.batch() for single-round-trip replay with per-frame data capture. No this.pyodide references remain in the method.</done>
</task>

<task type="auto">
  <name>Task 2: Migrate _performFastForward to batch and remove all shims</name>
  <files>
    interactive_gym/server/static/js/pyodide_multiplayer_game.js
    interactive_gym/server/static/js/PyodideWorker.js
    interactive_gym/server/static/js/pyodide_worker.js
    interactive_gym/server/static/js/pyodide_remote_game.js
  </files>
  <action>
**Part A: Migrate _performFastForward to worker.batch()**

Replace the runPythonAsync call in _performFastForward() (~line 5111) with a batch call.

The fast-forward is simpler than rollback -- it's just N steps with no snapshots needed:

```javascript
// Build batch of step operations
const batchOps = fastForwardFrames.map(ff => ({
    op: 'step',
    params: { actions: ff.actions }
}));

p2pLog.info(`FAST-FORWARD: batch stepping ${fastForwardFrames.length} frames`);

// Execute entire batch in single Worker round-trip
const batchResults = await this.worker.batch(batchOps);

// Process results - each is a step result
const perFrameData = [];
const ffCumulativeRewards = {};

for (let i = 0; i < fastForwardFrames.length; i++) {
    const ff = fastForwardFrames[i];
    const stepResult = batchResults[i];
    let { rewards, terminateds, truncateds, infos } = stepResult;

    // Convert to plain objects for storage
    const rewardsDict = typeof rewards === 'object' && !(rewards instanceof Map) ? rewards : (rewards instanceof Map ? Object.fromEntries(rewards) : { human: rewards });
    const termDict = typeof terminateds === 'object' && !(terminateds instanceof Map) ? terminateds : (terminateds instanceof Map ? Object.fromEntries(terminateds) : { human: terminateds });
    const truncDict = typeof truncateds === 'object' && !(truncateds instanceof Map) ? truncateds : (truncateds instanceof Map ? Object.fromEntries(truncateds) : { human: truncateds });
    const infosDict = typeof infos === 'object' && !(infos instanceof Map) ? infos : (infos instanceof Map ? Object.fromEntries(infos) : { human: infos });

    perFrameData.push({
        frame: ff.frame,
        actions: ff.actions,
        rewards: rewardsDict,
        terminateds: termDict,
        truncateds: truncDict,
        infos: infosDict
    });

    // Accumulate rewards
    for (const [agentId, reward] of Object.entries(rewardsDict)) {
        ffCumulativeRewards[agentId] = (ffCumulativeRewards[agentId] || 0) + reward;
    }
}
```

Then replace the existing result parsing (`const ffResult = JSON.parse(result);`) and the cumulative_rewards/per_frame_data processing with code that uses `perFrameData` and `ffCumulativeRewards` directly. Keep the existing frame data storage logic (termination frame check, storeFrameData, etc.) but feed it from the new data structures.

**Part B: Remove all backward-compatibility shims**

After confirming zero `this.pyodide` references remain in pyodide_multiplayer_game.js:

**B1. PyodideWorker.js -- Remove shim methods:**
- Remove the `runPythonAsync()` method (~lines 132-136) and its JSDoc
- Remove the `_wrapResult()` static method (~lines 147-160) and its JSDoc
- Remove the `toPy()` method (~lines 172-186) and its JSDoc

**B2. pyodide_worker.js -- Remove handleRunPython:**
- Remove the `handleRunPython()` function (~lines 288-305) and its JSDoc
- Remove the `case 'runPython':` from the handleMessage switch
- Update the module header comment to remove 'runPython' from the message types list

**B3. pyodide_remote_game.js -- Remove this.pyodide shim:**
- Remove the line `this.pyodide = this.worker;` (~line 64) in the initialize() method
- Remove the comment about the backward-compat shim above it (~3 lines of comment)

**IMPORTANT:** After shim removal, verify there are ZERO references to:
- `this.pyodide` in any JS file in the static/js directory
- `runPythonAsync` in PyodideWorker.js
- `toPy` in PyodideWorker.js
- `_wrapResult` in PyodideWorker.js
- `handleRunPython` in pyodide_worker.js
- `'runPython'` as a case in pyodide_worker.js handleMessage
  </action>
  <verify>
1. Grep for `this.pyodide` in pyodide_multiplayer_game.js -- must be ZERO
2. Grep for `this.pyodide` in pyodide_remote_game.js -- must be ZERO
3. Grep for `runPythonAsync` in PyodideWorker.js -- must be ZERO
4. Grep for `toPy` in PyodideWorker.js -- must be ZERO (as a method definition)
5. Grep for `_wrapResult` in PyodideWorker.js -- must be ZERO
6. Grep for `handleRunPython` in pyodide_worker.js -- must be ZERO
7. Grep for `runPython` in pyodide_worker.js -- must be ZERO (as a case or function)
8. Read _performFastForward and confirm uses worker.batch()
9. Read pyodide_remote_game.js initialize() and confirm no pyodide shim
10. Confirm all files parse (no syntax errors from removal)
  </verify>
  <done>_performFastForward uses worker.batch(). All Phase 68 shims removed: runPythonAsync, toPy, _wrapResult from PyodideWorker.js; handleRunPython from pyodide_worker.js; this.pyodide = this.worker from pyodide_remote_game.js. Zero this.pyodide references remain in any file.</done>
</task>

</tasks>

<verification>
1. `grep -r 'this\.pyodide' interactive_gym/server/static/js/*.js` -- must return ZERO matches
2. `grep -c 'worker\.batch' interactive_gym/server/static/js/pyodide_multiplayer_game.js` -- should be 2 (performRollback + _performFastForward)
3. `grep 'runPythonAsync\|handleRunPython\|_wrapResult' interactive_gym/server/static/js/PyodideWorker.js interactive_gym/server/static/js/pyodide_worker.js` -- must return ZERO
4. `grep 'toPy' interactive_gym/server/static/js/PyodideWorker.js` -- must return ZERO
5. Confirm all 4 modified files have no syntax errors (consistent braces, no dangling references)
</verification>

<success_criteria>
- performRollback uses this.worker.batch([setState ops skipped because loadStateSnapshot already called, ...computeHash+step pairs, getState]) in single round-trip
- _performFastForward uses this.worker.batch([...step ops]) in single round-trip
- Per-frame replay data (rewards, terminateds, truncateds, infos, actions) still captured during rollback for frameDataBuffer
- All 5 backward-compat shims removed (runPythonAsync, toPy, _wrapResult, handleRunPython, this.pyodide = this.worker)
- Zero this.pyodide references in entire codebase
- GGPO logic unchanged (rollback guard, snapshot management, input buffer, episode sync)
- INTEG-02 requirement satisfied: MultiplayerPyodideGame uses PyodideWorker for all Pyodide operations
</success_criteria>

<output>
After completion, create `.planning/phases/69-multiplayer-batch-operations/69-03-SUMMARY.md`
</output>
